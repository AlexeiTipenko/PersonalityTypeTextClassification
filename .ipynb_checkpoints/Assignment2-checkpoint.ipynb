{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i2X-7Q9A3LRd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/averyvine/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/averyvine/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/averyvine/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "from numpy.linalg import norm\n",
    "import math\n",
    "import time\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import string\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import gensim.downloader as api\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from matplotlib import pyplot as pl\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = string.punctuation + \"“”‘’—–…0123456789\\n\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "porter = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def __clean_review(review, rnn):\n",
    "    # Remove punctuation\n",
    "    review = review.translate(str.maketrans('', '', punctuation))\n",
    "    \n",
    "    # Tokenize\n",
    "    words = word_tokenize(review)\n",
    "    words = [word.lower() for word in words]\n",
    "    \n",
    "    # Remove stop words and gibberish\n",
    "    words = [word for word in words if (word.isalpha()) and (not word in stop_words)]\n",
    "    \n",
    "    # Apply stemming / lemmatization (only if not running RNN)\n",
    "    if not rnn:\n",
    "        # Apply stemming (if enabled, comment out lemmatization)\n",
    "#         words = [porter.stem(word) for word in words]\n",
    "        # Apply lemmatization (if enabled, comment out stemming)\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Rejoin words\n",
    "    review = \" \".join(words)\n",
    "    return review\n",
    "\n",
    "\n",
    "\n",
    "def clean_review_rnn(review):\n",
    "    return __clean_review(review, rnn=True)\n",
    "\n",
    "def clean_review(review):\n",
    "    return __clean_review(review, rnn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numberOfRows = 1000\n",
    "\n",
    "def getInitTrainDataFrame(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    # uncomment to limit training data\n",
    "    #df = df[0:numberOfRows]\n",
    "    print(\"Cleaning training review data...\")\n",
    "    df[\"review\"] = df[\"review\"].apply(cleanReview)\n",
    "    df[\"sentiment\"] = df[\"sentiment\"].apply(lambda sentiment: 0 if sentiment == \"negative\" else 1)\n",
    "    print(df)\n",
    "    return (df[\"review\"].values, df[\"sentiment\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfRows = 1000\n",
    "\n",
    "def getInitTestDataFrame(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    # uncomment to limit testing data\n",
    "    #df = df[0:numberOfRows]\n",
    "    print(\"Cleaning test review data...\")\n",
    "    df[\"review\"] = df[\"review\"].apply(cleanReview)\n",
    "    print(df)\n",
    "    return df[\"review\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainMatrix(reviewArr):\n",
    "    vectorizer = CountVectorizer(binary=True, max_features=5000, ngram_range=(1,1))\n",
    "    X = vectorizer.fit_transform(reviewArr)\n",
    "    return X, vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestMatrix(reviewArr, featureArr):\n",
    "    X = []\n",
    "    \n",
    "    for review in reviewArr:\n",
    "        featureDict = dict.fromkeys(featureArr, 0)\n",
    "        \n",
    "        for word in review.split():\n",
    "            if word in featureDict:\n",
    "                featureDict[word] = 1\n",
    "        \n",
    "        X.append(list(featureDict.values()))\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "        \n",
    "    # parameters needed for prediction, set in fit()\n",
    "    _theta_j_0 = None\n",
    "    _theta_j_1 = None\n",
    "    _log_theta = None\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        theta_1 = np.sum(y == 1) / float(len(y))\n",
    "        theta_0 = 1 - theta_1\n",
    "        self._log_theta = math.log(theta_1 / theta_0)\n",
    "        \n",
    "        n, m = X.shape\n",
    "        \n",
    "        totalMatrix = np.insert(X, m, y, axis=1)     # temporary, for easier vector manipulation \n",
    "        \n",
    "        totalMatrixY0 = totalMatrix[np.where(totalMatrix[:,-1]==0)]\n",
    "        totalMatrixY0 = np.delete(totalMatrixY0, -1, axis=1)\n",
    "        self._theta_j_0 = (np.sum(totalMatrixY0, axis=0)+1)/(float(np.sum(np.where(y==0))+2))\n",
    "        \n",
    "        totalMatrixY1 = totalMatrix[np.where(totalMatrix[:,-1]==1)]\n",
    "        totalMatrixY1 = np.delete(totalMatrixY1, -1, axis=1)\n",
    "        self._theta_j_1 = (np.sum(totalMatrixY1, axis=0)+1)/(float(np.sum(np.where(y==1))+2))\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        n,m = X.shape\n",
    "        \n",
    "        log_theta_j = np.log(self._theta_j_1 / self._theta_j_0)\n",
    "        log_negative_theta_j = np.log((1 - self._theta_j_1) / (1 - self._theta_j_0))\n",
    "        predictedLabels = self._log_theta + (np.matmul(X, log_theta_j) + np.matmul((1 - X), log_negative_theta_j))\n",
    "        \n",
    "        predictedLabels[np.where(predictedLabels>=0)] = 1\n",
    "        predictedLabels[np.where(predictedLabels<0)] = 0\n",
    "        \n",
    "        return predictedLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: predicted labels, true labels \n",
    "# Output: accuracy score\n",
    "def accuEval(y_predict, y_true):\n",
    "    \n",
    "    totalCorrect = np.sum(y_predict==y_true)\n",
    "    totalRows = len(y_predict)\n",
    "    \n",
    "    return(totalCorrect/totalRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download GloVe twitter set (separate cell for convenience)\n",
    "model = api.load(\"glove-twitter-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_lstm():\n",
    "    trainFile = \"train.csv\"\n",
    "    testFile = \"test.csv\"\n",
    "    \n",
    "    print(\"Getting initial training and rating arrays...\")\n",
    "    reviewsTrain, ratingsTrain = getInitTrainDataFrame(trainFile, rnn=True)\n",
    "    \n",
    "    print(\"Getting initial test array...\")\n",
    "    reviewsTest = getInitTestDataFrame(testFile, rnn=True)\n",
    "    \n",
    "    max_length = max([len(s.split()) for s in reviewsTrain])\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(reviewsTrain)\n",
    "    sequences = tokenizer.texts_to_sequences(reviewsTrain)\n",
    "    word_index = tokenizer.word_index\n",
    "    reviews_pad = pad_sequences(sequences, maxlen=max_length)\n",
    "    \n",
    "    indices = np.arange(reviews_pad.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    reviews_pad = reviews_pad[indices]\n",
    "    ratingsTrain = ratingsTrain[indices]\n",
    "\n",
    "    EMBEDDING_DIM = 100\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 50\n",
    "    \n",
    "    num_words = len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i > num_words or not word in model.vocab:\n",
    "            continue\n",
    "        embedding_vector = model[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    keras_model = Sequential()\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                embeddings_initializer=Constant(embedding_matrix),\n",
    "                                input_length=max_length,\n",
    "                                trainable=False)\n",
    "    keras_model.add(embedding_layer)\n",
    "    keras_model.add(LSTM(EMBEDDING_DIM, dropout=0.2, recurrent_dropout=0.2))\n",
    "    keras_model.add(Dense(1, activation='sigmoid'))\n",
    "    keras_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    history = keras_model.fit(reviews_pad, ratingsTrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS)\n",
    "    \n",
    "    sequences_test = tokenizer.texts_to_sequences(reviewsTest)\n",
    "    reviews_test_pad = pad_sequences(sequences_test, maxlen=max_length)\n",
    "    \n",
    "    predictions = keras_model.predict(reviews_test_pad, batch_size=BATCH_SIZE)\n",
    "    predictions[np.where(predictions>=0.5)] = 1\n",
    "    predictions[np.where(predictions<0.5)] = 0\n",
    "    np.asarray(predictions, dtype=int)\n",
    "    predict_df = pd.DataFrame(predictions, columns=['sentiment'])\n",
    "    predict_df.index.names = ['id']\n",
    "    predict_df[\"sentiment\"] = predict_df[\"sentiment\"].apply(lambda sentiment: \"negative\" if sentiment == 0 else \"positive\")\n",
    "    predict_df.to_csv(\"submission.csv\")\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting initial training and rating arrays...\n",
      "Cleaning training review data...\n",
      "                                                  review  sentiment\n",
      "0      never watched movie style filming may consider...          0\n",
      "1      robert lansing play scientist experimenting pa...          0\n",
      "2      looking forward movie trustworthy actor intere...          0\n",
      "3      start guy indian pot he cleaning suddenly skel...          0\n",
      "4      happened basically solid plausible premise dec...          0\n",
      "5      society heiress susan fletcher hopkins wealthy...          1\n",
      "6      yesterday went alone cinema mexico time movie ...          1\n",
      "7      first ever fully synchronized sound cartoon wa...          1\n",
      "8      watched movie second time enjoyed much first t...          1\n",
      "9      definitely outstanding musical great young sta...          1\n",
      "10     one film consider film rendition improvement o...          1\n",
      "11     transylvania insignificant occasionally funny ...          0\n",
      "12     much like orson welles thirty year earliermike...          0\n",
      "13     rating imdb understand people becoming upset t...          1\n",
      "14     awful couldt believe score annoying filming ba...          0\n",
      "15     meanwhile called loser exwife daughter cant pa...          1\n",
      "16     excellent story wonderful acting amazing produ...          1\n",
      "17     regular citizen work well loosing mentioned am...          0\n",
      "18     deliriously colossal vulgar silly star extrava...          1\n",
      "19     ive never seen original house wax really didnt...          1\n",
      "20     one solid randolph scott western play bat mast...          1\n",
      "21     earlier comment film garbo slightly different ...          1\n",
      "22     first reaction lot people left seeing shot fat...          1\n",
      "23     invisible mouse delightful different tom jerry...          1\n",
      "24     screen never painted blackandwhite case basene...          1\n",
      "25     expect lot movie terrible life miracle turn mo...          0\n",
      "26     sending critter space seem like entertaining i...          0\n",
      "27     unfortunately go nowherebr br despite impressi...          0\n",
      "28     gwoemul host due pollution han river mutated b...          0\n",
      "29     remember original series vividly mostly due un...          1\n",
      "...                                                  ...        ...\n",
      "29970  never really knew robert wuhl seeing seeing re...          1\n",
      "29971  cast list like one expected far better venessa...          0\n",
      "29972  dear peter seller one oddly talented actor cho...          0\n",
      "29973  lucille ball mighty power television throughou...          0\n",
      "29974  found gem rack local video rental store tape e...          0\n",
      "29975  person violating sanctity elicot didnt appear ...          0\n",
      "29976  love film excellent funny ben fit wouldnt mind...          1\n",
      "29977  yeah remember one many year since actually wat...          1\n",
      "29978  see director trying missed mark main actor rea...          0\n",
      "29979  parade ah moon honeymoon hotel waterfall last ...          1\n",
      "29980  undeveloped writer totally missed opportunity ...          0\n",
      "29981  read comment tomreynolds feel jump understand ...          1\n",
      "29982  trailer movie didnt movie justice movie didnt ...          1\n",
      "29983  rented movie video store last night new releas...          1\n",
      "29984  premise story simple old man living alone wood...          1\n",
      "29985  show incredibly dumb manchild shrewish hot wif...          0\n",
      "29986  quite astonished see people voted film write s...          1\n",
      "29987  replica insult samurai caste even morebr br al...          0\n",
      "29988  forgotten disaster moreover another problem pi...          0\n",
      "29989  total vixen vixen whose breast exposed through...          0\n",
      "29990  incredible performance one best film seen ever...          1\n",
      "29991  first saw live musical denver center performin...          0\n",
      "29992  saw recently cable channel movie great one mus...          1\n",
      "29993  writer lapsed orthodox jewish woman let tremen...          0\n",
      "29994  soulbr br special kudos mabel rivera ramons si...          1\n",
      "29995  saw uzumaki year ago mesmerized japanese horro...          1\n",
      "29996  sentinel hoping would good film boy righta gre...          1\n",
      "29997  blue really would look like set rural south dr...          1\n",
      "29998  really liked movie totally reminds high school...          1\n",
      "29999  plot scene furthermore chaplin film imitated g...          1\n",
      "\n",
      "[30000 rows x 2 columns]\n",
      "Getting training matrix and feature array...\n",
      "Getting initial test array...\n",
      "Cleaning test review data...\n",
      "        id                                             review\n",
      "0        0  deal issue nazism vilification german period w...\n",
      "1        1  br rambo realise set trautman portrayed brave ...\n",
      "2        2  found charming nobody else kiarostami little y...\n",
      "3        3  dont know stupid crap junk garbage good nothin...\n",
      "4        4  hey guy br br looking every find two movie can...\n",
      "5        5  elizabeth taylor never could act usual annoyin...\n",
      "6        6  loved movie almost first cabin lake instead ki...\n",
      "7        7  ok bad movie making original script notbr br p...\n",
      "8        8  ladens killed broken full absence mutual aid m...\n",
      "9        9  piece trash ever seen hour life stolen acting ...\n",
      "10      10  want start saying first review film done net f...\n",
      "11      11  shallow shallow script stilted acting shadow b...\n",
      "12      12  gun blasting building exploding car crashing t...\n",
      "13      13  love killer insect movie great fun watch watch...\n",
      "14      14  terrible there way get around script level one...\n",
      "15      15  small picturesque sicilian village someone bru...\n",
      "16      16  rock star welltold hollywoodstyle rendition ta...\n",
      "17      17  walt particularly fond quality come producer d...\n",
      "18      18  acquired reliable intel possible hit life davi...\n",
      "19      19  writing one chapter like serial opening one lo...\n",
      "20      20  truly one worst film ever seen life rod steige...\n",
      "21      21  hey wanted give crazy fact movie actually one ...\n",
      "22      22  first thing encounter mile end road colossal m...\n",
      "23      23  eye opening documentary siege religious sect c...\n",
      "24      24  recently watched version film judy appreciated...\n",
      "25      25  mayor hell feel early dead end kid film much h...\n",
      "26      26  hayao miyazakis movie always hitormiss regard ...\n",
      "27      27  great dictator beyondexcellent film charlie ch...\n",
      "28      28  water much merry mayhem ensuesbr br know mitch...\n",
      "29      29  oh gosh one best scifi movie ever seen quite f...\n",
      "...    ...                                                ...\n",
      "9970  9970  rather well done actuallyattack evil villain l...\n",
      "9971  9971  acting talent speak get bunch pretty boy tryin...\n",
      "9972  9972  kidnapping mexico common honey giving phone nu...\n",
      "9973  9973  found first minute film interesting even thoug...\n",
      "9974  9974  full house one worst tv series ever whybecause...\n",
      "9975  9975  martial artist jade screen acceptable job earl...\n",
      "9976  9976  club single bigoted television program history...\n",
      "9977  9977  addition wonder leave many clue note pad safe ...\n",
      "9978  9978  agree mr caruso jr lanzas finest voice god off...\n",
      "9979  9979  day never seen elizabeth shue anything else st...\n",
      "9980  9980  someone else mentioned begin bizarre prologue ...\n",
      "9981  9981  interesting idea storyline didnt quite workbr ...\n",
      "9982  9982  think one weakest kenneth branagh shakespearia...\n",
      "9983  9983  weekend ive watched funny film really like kid...\n",
      "9984  9984  saw movie last weekend silly mindless ancient ...\n",
      "9985  9985  hit memorable performance contrast well donova...\n",
      "9986  9986  film poorly casted except familiar old hollywo...\n",
      "9987  9987  saw hawaii film festival director wife produce...\n",
      "9988  9988  begun day time talk show sprout left right eve...\n",
      "9989  9989  first guinea pig series one infamous film coll...\n",
      "9990  9990  movie pile rubbish try base first farce main t...\n",
      "9991  9991  repressive perhaps good way meant none saturat...\n",
      "9992  9992  watched movie last night one payperview channe...\n",
      "9993  9993  find film speaks u many level story feel real ...\n",
      "9994  9994  tried remove anything might considered spoiler...\n",
      "9995  9995  movie comedy nowadays generally minute toilet ...\n",
      "9996  9996  cant say whether larry hama ever saw old carto...\n",
      "9997  9997  completely unbelievable lower deck type worshi...\n",
      "9998  9998  actually trilogy somerset maugham short tale f...\n",
      "9999  9999  movie bad laughable couldnt resist watching th...\n",
      "\n",
      "[10000 rows x 2 columns]\n",
      "Getting test matrix...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PCA on training data...\n",
      "PCA explained variance values:\n",
      "[0.6  0.39 0.29 0.27 0.26 0.25 0.24 0.23 0.23 0.22 0.21 0.21 0.2  0.2\n",
      " 0.2  0.19 0.19 0.18 0.18 0.18 0.18 0.17 0.17 0.17 0.17 0.17 0.17 0.16\n",
      " 0.16 0.16 0.15 0.15 0.15 0.15 0.14 0.14 0.14 0.14 0.14 0.14 0.13 0.13\n",
      " 0.13 0.13 0.13 0.13 0.13 0.13 0.12 0.12]\n",
      "Running k-fold cross validation...\n",
      "\n",
      "\n",
      "\n",
      "K-FOLD RUN 1\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for sklearn LR with PCA:  0.769\n",
      "Runtime: 9.579733848571777 seconds.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "K-FOLD RUN 2\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for sklearn LR with PCA:  0.7765\n",
      "Runtime: 12.188390970230103 seconds.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "K-FOLD RUN 3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for sklearn LR with PCA:  0.7673333333333333\n",
      "Runtime: 14.86371397972107 seconds.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "K-FOLD RUN 4\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for sklearn LR with PCA:  0.7535\n",
      "Runtime: 13.019276142120361 seconds.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "K-FOLD RUN 5\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for sklearn LR with PCA:  0.7696666666666667\n",
      "Runtime: 10.266211032867432 seconds.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    trainFile = \"train.csv\"\n",
    "    testFile = \"test.csv\"\n",
    "    \n",
    "    print(\"Getting initial training and rating arrays...\")\n",
    "    reviewsTrain, ratingsTrain = getInitTrainDataFrame(trainFile)\n",
    "    \n",
    "    print(\"Getting training matrix and feature array...\")\n",
    "    X_train, featureArr = getTrainMatrix(reviewsTrain)\n",
    "    \n",
    "    print(\"Getting initial test array...\")\n",
    "    reviewsTest = getInitTestDataFrame(testFile)\n",
    "    \n",
    "    print(\"Getting test matrix...\")\n",
    "    X_test = getTestMatrix(reviewsTest, featureArr)\n",
    "    \n",
    "    X_train = np.asarray(X_train.toarray())\n",
    "    y_train = np.asarray(ratingsTrain, dtype=int)\n",
    "    X_test = np.asarray(X_test)\n",
    "    \n",
    "    print(\"Running PCA on training data...\")\n",
    "    pca = PCA(n_components=50)\n",
    "    pca.fit(X_train)\n",
    "    X_transformed = pca.transform(X_train)\n",
    "    print(\"PCA explained variance values:\")\n",
    "    print(pca.explained_variance_.round(2))\n",
    "    \n",
    "    \n",
    "    print(\"Running k-fold cross validation...\\n\")\n",
    "    kFoldRun = 0\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    for train_index, validation_index in kf.split(X_train):\n",
    "        \n",
    "        kFoldRun = kFoldRun + 1\n",
    "        print(\"\\n\")\n",
    "        print(\"K-FOLD RUN %s\" % kFoldRun)\n",
    "        print(\"\\n\")\n",
    "        X_train_split, X_valid_split = X_train[train_index], X_train[validation_index]\n",
    "        y_train_split, y_valid_split = y_train[train_index], y_train[validation_index]\n",
    "        \n",
    "#         start_time = time.time()\n",
    "#         nb = NaiveBayes()\n",
    "#         nb.fit(X_train_split, y_train_split)\n",
    "#         ratingsPredicted = nb.predict(X_valid_split)\n",
    "#         print(\"Accuracy for naive bayes: \", accuEval(ratingsPredicted, y_valid_split))\n",
    "#         print(\"Runtime: %s seconds.\" % (time.time() - start_time))\n",
    "#         print(\"\\n\")\n",
    "        \n",
    "#         start_time = time.time()\n",
    "#         nbSK = BernoulliNB()\n",
    "#         nbSK.fit(X_train_split, y_train_split)\n",
    "#         ratingsPredictedSK = nbSK.predict(X_valid_split)\n",
    "#         print(\"Accuracy for sklearn naive bayes: \", accuEval(ratingsPredictedSK, y_valid_split))\n",
    "#         print(\"Runtime: %s seconds.\" % (time.time() - start_time))\n",
    "#         print(\"\\n\")\n",
    "        \n",
    "#         start_time = time.time()\n",
    "#         decTree = DecisionTreeClassifier(criterion='entropy', max_depth=15, min_samples_split=20, min_samples_leaf=10)\n",
    "#         decTree.fit(X_train_split, y_train_split)\n",
    "#         ratingsPredictedTree = decTree.predict(X_valid_split)\n",
    "#         print(\"Accuracy for sklearn decision tree: \", accuEval(ratingsPredictedTree, y_valid_split))\n",
    "#         print(\"Runtime: %s seconds.\" % (time.time() - start_time))\n",
    "#         print(\"\\n\")\n",
    "        \n",
    "#         start_time = time.time()\n",
    "#         lda = LinearDiscriminantAnalysis()\n",
    "#         lda.fit(X_train_split, y_train_split)\n",
    "#         ratingsPredictedLDA = lda.predict(X_valid_split)\n",
    "#         print(\"Accuracy for sklearn LDA: \", accuEval(ratingsPredictedLDA, y_valid_split))\n",
    "#         print(\"Runtime: %s seconds.\" % (time.time() - start_time))\n",
    "#         print(\"\\n\")\n",
    "        \n",
    "#         start_time = time.time()\n",
    "#         qda = QuadraticDiscriminantAnalysis()\n",
    "#         qda.fit(X_train_split, y_train_split)\n",
    "#         ratingsPredictedQDA = qda.predict(X_valid_split)\n",
    "#         print(\"Accuracy for sklearn QDA: \", accuEval(ratingsPredictedQDA, y_valid_split))\n",
    "#         print(\"Runtime: %s seconds.\" % (time.time() - start_time))\n",
    "#         print(\"\\n\")\n",
    "\n",
    "#         start_time = time.time()\n",
    "#         lr = LogisticRegression()\n",
    "#         lr.fit(X_train_split, y_train_split)\n",
    "#         ratingsPredictedLR = lr.predict(X_valid_split)\n",
    "#         print(\"Accuracy for sklearn LR: \", accuEval(ratingsPredictedLR, y_valid_split))\n",
    "#         print(\"Runtime: %s seconds.\" % (time.time() - start_time))\n",
    "#         print(\"\\n\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        pca = PCA(n_components=50)\n",
    "        lr = LogisticRegression()\n",
    "        pipe = Pipeline([('pca', pca), ('logistic', lr)])\n",
    "        pipe.fit(X_train_split, y_train_split)\n",
    "        ratingsPredictedLR = pipe.predict(X_valid_split)\n",
    "        print(\"Accuracy for sklearn LR with PCA: \", accuEval(ratingsPredictedLR, y_valid_split))\n",
    "        print(\"Runtime: %s seconds.\" % (time.time() - start_time))\n",
    "        print(\"\\n\\n\\n\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#     print(\"Running predictions on test data for LR...\\n\")\n",
    "    \n",
    "    \n",
    "    #nb = NaiveBayes()\n",
    "    #nb.fit(X_train, y_train)\n",
    "    #ratingsPredicted = nb.predict(X_test)\n",
    "    \n",
    "    #pca = PCA(n_components=500)\n",
    "    #pipe = Pipeline([('pca', pca), ('logistic', lr)])\n",
    "    \n",
    "#     lr = LogisticRegression()\n",
    "#     lr.fit(X_train, y_train)\n",
    "#     ratingsPredicted = lr.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#     predict_df = pd.DataFrame(ratingsPredictedLDA, columns=['sentiment'])\n",
    "#     predict_df.index.names = ['id']\n",
    "#     predict_df[\"sentiment\"] = predict_df[\"sentiment\"].apply(lambda sentiment: \"negative\" if sentiment == 0 else \"positive\")\n",
    "#     predict_df.to_csv(\"submission.csv\")\n",
    "#     print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Uncomment one or the other (or both, if you feel like waiting!)\n",
    "\n",
    "main()\n",
    "# main_lstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparamTuning():\n",
    "    \n",
    "    trainFile = \"train.csv\"\n",
    "    testFile = \"test.csv\"\n",
    "    \n",
    "    print(\"Getting initial training and rating arrays...\")\n",
    "    reviewsTrain, ratingsTrain = getInitTrainDataFrame(trainFile)\n",
    "    \n",
    "    print(\"Getting training matrix and feature array...\")\n",
    "    X_train, featureArr = getTrainMatrix(reviewsTrain)\n",
    "    \n",
    "    X_train = np.asarray(X_train.toarray())\n",
    "    y_train = np.asarray(ratingsTrain, dtype=int)\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.25)\n",
    "    \n",
    "    #max_depths = np.linspace(1, 32, 32, endpoint=True)\n",
    "    #min_samples_splits = np.linspace(2, 100, 20, endpoint=True, dtype=int)\n",
    "    min_samples_leafs = np.linspace(2, 100, 20, endpoint=True, dtype=int)\n",
    "    #max_features = list(range(1,X_train.shape[1]))\n",
    "    \n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    \n",
    "    #for max_depth in max_depths:\n",
    "    #for min_samples_split in min_samples_splits:\n",
    "    for min_samples_leaf in min_samples_leafs:\n",
    "    #for max_feature in max_features:\n",
    "        \n",
    "        #dt = DecisionTreeClassifier(max_depth=max_depth)\n",
    "        #dt = DecisionTreeClassifier(min_samples_split=min_samples_split)\n",
    "        dt = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf)\n",
    "        #dt = DecisionTreeClassifier(max_features=max_feature)\n",
    "        \n",
    "        dt.fit(X_train, y_train)   \n",
    "        train_pred = dt.predict(X_train)   \n",
    "        false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n",
    "        roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "        # Add auc score to previous train results\n",
    "        train_results.append(roc_auc)   \n",
    "        y_pred = dt.predict(X_test)   \n",
    "        false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "        roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "        # Add auc score to previous test results\n",
    "        test_results.append(roc_auc)\n",
    "\n",
    "    from matplotlib.legend_handler import HandlerLine2D\n",
    "    \n",
    "    #line1, = plt.plot(max_depths, train_results, 'b', label='Train AUC')\n",
    "    #line2, = plt.plot(max_depths, test_results, 'r', label='Test AUC')\n",
    "    \n",
    "    #line1, = plt.plot(min_samples_splits, train_results, 'b', label='Train AUC')\n",
    "    #line2, = plt.plot(min_samples_splits, test_results, 'r', label='Test AUC')\n",
    "    \n",
    "    line1, = plt.plot(min_samples_leafs, train_results, 'b', label='Train AUC')\n",
    "    line2, = plt.plot(min_samples_leafs, test_results, 'r', label='Test AUC')\n",
    "    \n",
    "    #line1, = plt.plot(max_features, train_results, 'b', label='Train AUC')\n",
    "    #line2, = plt.plot(max_features, test_results, 'r', label='Test AUC')\n",
    "    \n",
    "    plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "    plt.ylabel('AUC score')\n",
    "\n",
    "    #plt.xlabel('Tree depth')\n",
    "    #plt.xlabel('min samples split')\n",
    "    plt.xlabel('min samples leaf')\n",
    "    #plt.xlabel('max features')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hyperparamTuning()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
